Hardware virtualization has become mainstream technology over the last decade as it allows
to host more than one operating system on a single server and to strictly
separate users of software environments.
Hardware and software stacks are decoupled, such that complete software
environments can be migrated across hardware boundaries.
While widespread in computer center
operation this technique is rarely applied in HPC.
% it is still skeptically seen in the field of scientific computing~\cite{VirtualisationScientificComp} and
% thus rarely applied in HPC.

\subsection{Computing at the University of Freiburg}
% Infrastructure setting

The computer center at the University of Freiburg provides
medium scale research
infrastructures like cloud, storage, and especially HPC services adapted to the
needs of various scientific communities. Significant standardization
in hardware and software is necessary for the operation of compute systems comprised of
more than 1000 individual nodes by a small group of administrators.

To support whole research environments wich are required by world-wide efforts like the
ATLAS or CMS experiments, novel approaches are necessary to ensure optimal use of the system
and to open the cluster to as many different use-cases as
possible without increasing the operational effort.
Transferring expertise from the operation of the established local
private cloud, %in cooperation with the ViCE project~\cite{vice},
the use of \Openstack as a cloud platform has been identified
as a
suitable solution for \NEMO. This approach provides a user defined software
deployment in addition to the existing software modules environment used for
providing different kinds of software packages \cite{online:modules}.
The resulting challenges range from the automated creation of suitable
virtual machines to their on-demand deployment and scheduling.

\subsection{Research Cluster \NEMO}

The research cluster \NEMO is a cluster for 
research in the federal state of Baden-W\"urttemberg in the scientific fields of Elementary Particle Physics, Neuroscience and
Microsystems Engineering. Operation started on  August 1, 2016.
It currently consists of 900 nodes with 20 physical cores and 128\,GiB of RAM each.
Omni-Path~\cite{Omnipath} provides a high-speed, low-latency network of 100\,Gbit/s between nodes.
The parallel storage has
768 TB of usable capacity and is based on \BeeGFS~\cite{BeeGFS}.
%In October 2017 the \NEMO cluster was extended by research groups and offers
%currently 900 nodes and a total capacity of 768\,TB of parallel storage.

A pre-requirement to execute a VRE is the efficient
provisioning of data which has to cross institutional boundaries in the CMS use-case.
A signficant bandwidth is needed to transfer the input data into the VRE from the storage system at
the Karlsruhe Institute of Technology (KIT) and to store back the results. The
NEMO cluster is connected with two 40\,Gbit/s links to the main router of the
University of Freiburg which itself is linked to the network of
scientific institutions in Baden-W\"urttemberg, BelW\"u, at
100\,Gbit/s.

\subsection{Separation of software environments}

The file system of a VRE is a
disk image presented as a single file. From the computer center's perspective
this image is a ``black box'' requiring no involvement or efforts like
updates of the operating system or the provisioning of software packages of a
certain version. From the researcher's perspective the VRE is an individual
virtual node whose operating system, applications and configurations
as well as certain hardware-level parameters, e.g. CPU and RAM, can be
configured fully autonomously by the researcher within agreed upon limits.


To increase the flexibility in hosted software environments, the standard bare metal
operation of \NEMO is extended with an installation of \Openstack
components~\cite{hpc-symp:2016}.
The \NEMO cluster uses Adaptive's Workload Manager \Moab~\cite{Moab} as a
scheduler of compute jobs.
\Openstack as well can schedule virtual machines on the same nodes and
resources.
To avoid conflicts, it is necessary to define the master scheduler
which decides the job assignment to the worker nodes.
Both \Moab and \Openstack are
unaware that another scheduler exists within the cluster and there is
no API which enables them to  communicate with each other. Since the majority of users still use the
bare metal HPC cluster, \Moab is deployed as the primary scheduler. It allows for
detailed job description and offers sophisticated scheduling features like
fair-share, priority-based scheduling, detailed time limits,
etc. \Openstack 's task is to deploy the virtual machines, but \Moab will initially start the VRE
jobs and the VRE job will instruct \Openstack to start the virtual machine on the
reserved resources with the required flavor, i.e. according to the resource definition in \Openstack.

A VRE job is like any other bare-atal job.
When a VRE job is submitted to the \NEMO cluster, \Moab first calculates the
priority and the needed resources of the job and then inserts it into its queue.
If the VRE job is in line for execution and the requested resources are available,
the job runs a script which starts the VRE (a virtual machine in our case) on the selected node
within the resource boundaries. This can be compared to a bare-metal job script starting a scientific software.
During the run-time of the virtual machine (VRE) the job runs a monitoring
script which regularly checks if the virtual machine is still running.
When the VRE teminates by an internal shutdown of the virtal machine the VRE job ends, as well.
If the VRE job ends e.g. when it hits the wall-time, \Openstack gets a signal to terminate the virtual machine and
the VRE job ends as well. Neither \Moab nor \Openstack have access
inside the VRE, so they cannot assess if the VRE is actually busy or
idle.
The software package \Roced (described in
further detail in Section~\ref{section:roced}) has been introduced to
solve this issue.
It is used as a broker between
different HPC schedulers,  translating resources and monitoring usage inside the
virtual machine, as well as starting and stopping VRE images on demand.
