%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
\usepackage{url}
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Dynamic Virtualized Deployment of Particle Physics Environments on a
  High Performance Computing Cluster%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{Felix B\"uhrer \and Anton Gamel  \and Michael Janczyk \and
  Benoit Roland \and
  Markus Schumacher \and
  Ulrike Schnoor \and Bernd Wiebelt \and Thomas Hauth
  % etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{U. Schnoor \at
              CERN \\
              \email{ulrike.schnoor@cern.ch}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Particle physics experiments at the Large Hadron Collider (LHC) need a
great quantity of computing resources for data processing, simulation, and analysis.
High Performance Computing (HPC) resources provided by research institutions
can be useful supplements to the existing World-wide LHC Computing
Grid (WLCG) resources
allocated by the collaborations. At the University of Frei\-burg, the
shared research cluster NEMO has been made available to ATLAS and CMS
users accessing it from external machines connected to the WLCG.
 To this effect, the full software environment corresponding to a WLCG center
 is provided in a virtual machine image. The interplay between the
 schedulers for NEMO and for the external
 cluster is ensured through the ROCED service.
An OpenStack infrastructure is deployed at NEMO to orchestrate the
simultaneous usage by bare metal and virtualized jobs.
Through the setup, resources are provided to users in a transparent,
automatized, and
on-demand way. The performance of the virtualized environment has been
evaluated for particle physics applications.

%Insert your abstract here. Include keywords, PACS and mathematical
%subject classification numbers as needed.
\keywords{Virtualization \and Particle Physics \and Grid Computing }
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}




\section{Introduction}
\label{intro}
\input{include/Introduction.tex}

\section{Virtualization infrastructure}
\label{sec:openstack}

\input{include/Infrastructure.tex}
\section{Generation of the image}
The VREs for ATLAS and CMS software environments are simply OpenStack containers
in the format of a compatible VM image.
These images are provided in an automatized
way allowing versioning and archiving of the environments captured in
the images.
% The approaches used in the
% different groups are described in the following.

\subsection{Packer combined with Puppet}

\input{include/PackerPuppet.tex}



\subsection{Image generation using the Oz toolkit}
%$\to$CMS Karlsruhe\\
\input{include/KarlsruheImageGen.tex}

\section{Interfacing batch systems and virtual resources using ROCED}
\label{section:roced}
While virtualized HPC systems and commercial cloud offerings provide the necessities to acquire computing and storage capacity by dynamic resource booking, the computing needs of high energy physics re\-search groups ad\-di\-tion\-al\-ly require work\-flow ma\-na\-ge\-ment sys\-tems capable of maintaining thousands of batch jobs. While some cloud providers, for example Amazon with AWS Batch~\cite{awsbatch}, provide a service for workflow management, these offerings are often limited to one specific cloud instance. To dynamically distribute batch jobs to multiple sites and manage machine life-time on specific sites, a combination of a highly-scalabe batch system and a virtual machine scheduler is desirable.

\subsection{ROCED}
\begin{figure*}
\begin{center}
  \includegraphics[width=0.9\linewidth]{figures/roced_design_flat.pdf}
  \caption{Overview of the ROCED modular design. The ROCED Core contains the Broker which decides when and on which sites new virtual machines are booted. The Requirement Adapters report about the utilization and resource requirements of the attached batch systems. The Site Adapter is responsible to manage the lifetime of virtual machines on an cloud site and the Integration Adapter ensure that newly booted machines are integrated into the batch system.}
  \label{fig-roced}
\end{center}
\end{figure*}

%$\to$ CMS Karlsruhe
Many capable batch systems exist today and they can be interfaced to virtualization providers using the cloud meta-scheduler ROCED (Responsive On-demand Cloud Enabled Deployment) which has been developed at the KIT since 2010~\cite{ROCED}. ROCED is written in a modular
fashion in python and the interfaces to batch systems and cloud sites are implemented as so-called \textit{Adapters}. This makes ROCED independent of specific user groups or workflows. It provides a scheduling core which collects the current requirement of computing resources and decides if virtual machines need to be started or can be stopped. One or more Requirement Adapters report the current queue status of batch systems to the central scheduling core. Currently, Requirement Adapters are implemented for the Slurm, Torque, HTCondor and GridEngine batch systems. The Site Adapters allow ROCED to start, stop and monitor virtual machines on multiple cloud sites. Implementations exist for Amazon EC2, OpenStack, OpenNebula and Moab-based virtualization at HPC centers. Special care has been put into the resilience of ROCED: it can automatically terminate non-responsive machines and restart virtual machines in case some machines dropped out. This allows VM setups orchestrated by ROCED with thousands of virtual machines and many tens of thousands of jobs to run in production environments.
The modular design of ROCED is shown in fig.~\ref{fig-roced}.

\subsection{Using HTCondor as front-end scheduler}\label{sec:ROCED:HTCondor}
%$\to$ CMS Karlsruhe
\input{include/HTCondor.tex}

\subsection{Using SLURM as front-end scheduler}
%$\to$ ATLAS Freiburg\\
\input{include/Slurm.tex}


\section{Analysis of performance and usage}

This apporach has been implemented and put into production in the research groups at University of Freiburg (Physikalisches Institut) and the Karlsruhe Institute of Technology (Institute of Experimental Particle Physics). The following chapter presents the statistical analysis of the performance of the virtualized setup both in terms of job performance as well as usage statistics.

\subsection{HEPSPEC benchmarks}
%$\to$ ATLAS Freiburg\\
\input{include/Hepspec.tex}



%\subsection{Production of simulation data}
%%$\to$ATLAS Freiburg
%\input{include/Simulationtests.tex}
%
%\subsection{Data analysis}
%$\to$ATLAS Freiburg 
%
\subsection{Usage statistics}
Fig. \ref{fig-frplots} show the utilization of virtual machines which were orchestrated by ROCED depending on the resource demands of the users of the KIT group.
At peak times, up to 9000 virtual cores were filled with user jobs, consuming more than a half of the initial 16000 NEMO cores.

\begin{figure}
\begin{center}
  \includegraphics[width=1.0\linewidth]{figures/NEMO_KIT_utiliztion.pdf}
  \caption{Utilization of the shared HPC system by booted virtual machines. Up to 9000 virtual cores were in use at peak times. The fluctuations in the utilization reflects the patterns of the submission of jobs by our institute users. The number of draining slots displays the amount of job slots still processing jobs while the rest of the node's slot are already empty.}
  \label{fig-frplots}
\end{center}
\end{figure}

The usage of the hybrid cluster model is presented in fig.~\ref{fig-nodeusage}.
The diagram shows the shared usage of NEMO's cluster nodes running either
bare-metal or virtualized jobs. The part of the cluster which runs virtualized
jobs or VREs changes dynamically from job to job, since the VREs are started by
a standard bare-metal job.

At the beginning the cluster was only containing the operating system and some
basic development tools, scientific software was added after the the cluster was
already in production mode. Since the VRE for the CMS project was already
available when the NEMO cluster started it could already use the whole cluster
while other groups still had to wait for the required scientific software to be deployed on
the cluster. It explains the high usage by VREs in the first months of
operation. With more and more software being available for bare-metal usage the
amount of VRE jobs decreased. This figure is only an estimate because VRE
projects are not forced to use VREs and therefore could run bare-metal jobs as
well.

% The shared usage of the NEMO cluster between jobs running in virtual
% machines and within the cluster operating system is illustrated in
% Figure~\ref{fig-nodeusage}. It shows the relative contributions of CPU hours used by jobs running
% direcly in the hosts' operating system or in virtual machines per month.
% (Ist das wirklich so angegeben?)
% The figure shows that the shares of bare-metal and virtualized jobs is
% completely flexible and can be adapted to the current needs and
% priorities of the user community.


\begin{figure}
\begin{center}
  \includegraphics[width=0.9\linewidth]{figures/NodeUsage_2016-09_2018-06.png}
  \caption{Usage of the NEMO cluster in the time between September 2016
    to June 2018. The blue bars indicate the usage by jobs
    running directly in the hosts' operating system, while the orange bars are jobs
    running in virtual machines.}
  \label{fig-nodeusage}
\end{center}
\end{figure}

\section{Conclusions and Outlook}
\input{include/Conclusions}

%%% REDUNDANT
% A system for the dynamic, on-demand provisioning of virtual machines
% to run jobs in a high energy physics context on an external, not
% dedicated resource as realized at the HPC
% cluster NEMO at University of Freiburg has been described.
% Reasons for the need for an interface between the schedulers of the host system
% and the external system from which requests are sent have been
% explained.
% The performance and usage have been analyzed for selected cases.
%
% This approach can be generalized to other platforms and possibly also
% other forms of virtualized environments (containers).
















%\subsection{Subsection title}
%\label{sec:2}
%as required. Don't forget to give each section
%and subsection a unique label (see Sect.~\ref{sec:1}).

%\paragraph{Paragraph headings} Use paragraph headings as needed.
%\begin{equation}
%a^2+b^2=c^2
%\end{equation}
%
%% For one-column wide figures use
%\begin{figure}
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
%  \includegraphics{example.eps}
%% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:1}       % Give a unique label
%\end{figure}
%%
%% For two-column wide figures use
%\begin{figure*}
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
%  \includegraphics[width=0.75\textwidth]{example.eps}
%% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:2}       % Give a unique label
%\end{figure*}
%
%% For tables use
%\begin{table}
%% table caption is above the table
%\caption{Please write your table caption here}
%\label{tab:1}       % Give a unique label
%% For LaTeX tables use
%\begin{tabular}{lll}
%\hline\noalign{\smallskip}
%first & second & third  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%number & number & number \\
%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}


%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

% Non-BibTeX users please use
\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
\bibitem{Openstack}
OpenStack Open Source Cloud Computing Software
\url{https://www.openstack.org/}, accessed 2018-07-03

\bibitem{ROCED}
ROCED Cloud Meta-Scheduler project website
\url{https://github.com/roced-scheduler/ROCED}, accessed 2018-07-03

\bibitem{Moab}
Adaptive Computing Moab
\url{http://www.adaptivecomputing.com/moab-hpc-basic-edition/}, accessed 2018-07-03

% Commented out, because reference is not used in text currently
%\bibitem{VirtualisationScientificComp}
%  This needs a reference

\bibitem{OZ}
Oz image generation toolkit
\url{https://github.com/clalancette/oz}, accessed 2018-07-03

\bibitem{awsbatch}
Amazon AWS Batch
\url{https://aws.amazon.com/batch/}, accessed 2018-07-03

\bibitem{HTCondor}
HTCondor workload manager
\url{https://research.cs.wisc.edu/htcondor/}, accessed 2018-07-03

\bibitem{HTCondorCCB}
HTCondor Connection Brokering
\url{http://research.cs.wisc.edu/htcondor/manual/v8.6/3_9Networking_includes.html}, accessed 2018-07-03

\bibitem{packer}

Packer: tool for creating machine and container images for multiple platforms from a single source configuration. 
\url{https://www.packer.io/}, accessed 2018-07-03

\bibitem{kickstart}
\url{https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/installation_guide/ch-kickstart2}, accessed 2018-07-03

\bibitem{puppet}

Puppet Enterprise. ``IT automation for cloud, security, and DevOps.''
\url{https://puppet.com/}, accessed 2018-07-03

\bibitem{SlurmElastic}
Slurm Elastic Computing
\url{https://slurm.schedmd.com/elastic_computing.html}, accessed 2018-07-03

\bibitem{hpc-symp:2016}
Dirk von Suchodoletz, Bernd Wiebelt, Konrad Meier, Michael Janczyk,
  Flexible HPC: bwForCluster NEMO,
  Proceedings of the 3rd bwHPC-Symposium: Heidelberg 2016,
  \url{http://books.ub.uni-heidelberg.de/heibooks/reader/download/308/308-4-79237-1-10-20171002.pdf}

\bibitem{Hepspec} HEPiX Benchmarking Working Group:
\url{https://twiki.cern.ch/twiki/bin/view/FIOgroup/TsiBenchHEPSPEC}, accessed 2018-01-29

\bibitem{PowhegBox}
   P. Nason, JHEP 0411 (2004) 040, hep-ph/0409146;
    S. Frixione, P. Nason and C. Oleari, JHEP 0711 (2007) 070, arXiv:0709.2092;
    S. Alioli, P. Nason, C. Oleari and E. Re, JHEP 1006 (2010) 043, arXiv:1002.2581

\bibitem{Pythia8}

T. Sj√∂strand et al: An Introduction to PYTHIA 8.2. Comput. Phys. Commun. 191 (2015) 159-177. DOI:10.1016/j.cpc.2015.01.024". arXiv hep-ph 1410.3012

\bibitem{vice}
TODO

%title={Flexible HPC: bwForCluster NEMO},
%authors={Dirk von Suchodoletz and Bernd Wiebelt and Konrad Meier and
%Michael Janczyk},
%editors={Richling, Sabine and Baumann, Martin and Heuveline, Vincent},
%booktitle={Proceedings of the 3rd bwHPC-Symposium: Heidelberg 2016},
%publisher={heiBOOKS},
%year={2017},
%DOI={10.11588/heibooks.308.418},
%url={http://books.ub.uni
%heidelberg.de/heibooks/reader/download/308/308-4-79237-1-10-20171002.pdf}
%}

%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
% etc
\end{thebibliography}

\end{document}
% end of file template.tex

