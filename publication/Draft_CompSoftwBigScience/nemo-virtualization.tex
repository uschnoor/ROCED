%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{hyperref}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
\usepackage{url}
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
\newcommand{\Openstack}{\texttt{Open\-Stack}\xspace}
\newcommand{\Roced}{{ROCED}\xspace}
\newcommand{\ATLAS}{ATLAS\xspace}
\newcommand{\CMS}{CMS\xspace}
\newcommand{\NEMO}{NEMO\xspace}
\newcommand{\Packer}{\texttt{Packer}\xspace}
\newcommand{\Puppet}{\texttt{Puppet}\xspace}
\newcommand{\Hieradata}{\texttt{Hieradata}\xspace}
\newcommand{\Moab}{\texttt{Moab}\xspace}
\newcommand{\Slurm}{\texttt{Slurm}\xspace}
\newcommand{\BeeGFS}{\textsc{BeeGFS}\xspace}
\newcommand{\OZ}{\texttt{Oz}\xspace}
\newcommand{\HTCondor}{\texttt{HTCondor}\xspace}
\newcommand{\kickstart}{\texttt{kickstart}\xspace}
\newcommand{\sinfo}{\texttt{sinfo}\xspace}

\hyphenation{
phys-ics
re-search
start-ed
}




% Insert the name of "your journal" with
\journalname{Computing and Software for Big Science}
%
\begin{document}

\title{Dynamic Virtualized Deployment of Particle Physics Environments on a
  High Performance Computing Cluster}
%
\author{Felix B\"uhrer \and Frank Fischer \and Georg Fleig \and Anton
  Gamel \and Manuel Giffels \and Thomas Hauth \and Michael Janczyk
  \and Konrad Meier \and
G\"unter Quast \and  Beno\^it Roland \and
  Markus Schumacher \and Ulrike Schnoor \and Dirk von Suchodoletz \and Bernd Wiebelt
}



\institute{
   U. Schnoor \at   Universit\"at Freiburg, Physikalisches
  Institut, Hermann-Herder-Str. 3, 79104 Freiburg, Germany 
  \at \emph{Now at} CERN, CH-1211 Geneva
  23, Switzerland        %  \\
  \\\email{ulrike.schnoor@cern.ch}
  \and F. B\"uhrer \and B. Roland \and M. Schumacher \at Universit\"at Freiburg, Physikalisches
  Institut, Hermann-Herder-Str. 3, 79104 Freiburg, Germany
  \and A. Gamel  \at  Universit\"at Freiburg, Physikalisches
  Institut, Hermann-Herder-Str. 3, 79104 Freiburg, Germany \at  Universit\"at Freiburg, Rechenzentrum, Hermann-Herder-Str.10, 79104
  Freiburg, Germany
  \and M. Janczyk 
  \and K. Meier   \and D. von Suchodoletz 
 \and B. Wiebelt \at Universit\"at Freiburg, Rechenzentrum,
 Hermann-Herder-Str. 10, 79104
 Freiburg, Germany
  \and F. Fischer \and G. Fleig   \and M. Giffels  \and T. Hauth\and G. Quast  \at Karls\-ruher Institut f\"ur Technologie, Institut f\"ur Experimentelle
  Teilchenphysik, Wolfgang-Gaede-Str. 1, 76131 Karls\-ruhe, Germany
}



\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
The \NEMO High Performance Computing Cluster at the University of
Frei\-burg has been made available to
researchers of the \ATLAS and \CMS experiments.
Users access the cluster from external machines connected to the
World-wide LHC Computing Grid (WLCG).
 This paper describes how the full software environment of the WLCG
 is provided in a virtual machine image. The interplay between the
 schedulers for \NEMO and for the external
 clusters is coordinated through the \Roced service.
A cloud computing infrastructure is deployed at \NEMO to orchestrate the
simultaneous usage by bare metal and virtualized jobs.
Through the setup, resources are provided to users in a transparent,
automatized, and
on-demand way. The performance of the virtualized environment has been
evaluated for particle physics applications.


\keywords{Virtualization \and Particle Physics \and Grid Computing
  \and Benchmarks \and Opportunistic Usage}

\end{abstract}




\section{Introduction}
\label{intro}
\input{include/Introduction.tex}

\section{Virtualization infrastructure}
\label{sec:openstack}

\input{include/Infrastructure.tex}
\section{Generation of the VRE image}
The VREs for ATLAS and CMS software environments consist in \Openstack compatible VM images.
These virtual machines are provided in an automatized
way allowing versioning and archiving of the environments captured in
the images.
% The approaches used in the
% different groups are described in the following.

\subsection{\Packer combined with \Puppet}

\input{include/PackerPuppet.tex}



\subsection{Image generation using the \OZ toolkit}
%$\to$CMS Karlsruhe\\
\input{include/KarlsruheImageGen.tex}




\section{Interfacing batch systems and virtual resources using \Roced}
\label{section:roced}
While HPC systems with support for virtualized research environments and commercial cloud providers offer the
necessities to acquire computing and storage capacity by dynamic
resource booking, the computing needs of high energy physics
re\-search groups ad\-di\-tion\-al\-ly require work\-flow
ma\-na\-ge\-ment sys\-tems capable of maintaining thousands of batch
jobs. Some cloud providers, for example Amazon with AWS
Batch~\cite{awsbatch}, provide a service for workflow management,
however these offers are often limited to one specific cloud instance. To dynamically distribute batch jobs to multiple sites and manage machine life-time on specific sites, a combination of a highly-scalable batch system and a virtual machine scheduler is desirable.

\subsection{\Roced}
\begin{figure*}
\begin{center}
  \includegraphics[width=0.9\linewidth]{figures/roced_design_flat.pdf}
  \caption{Overview of the \Roced modular design. The  \Roced Core contains the Broker which decides when and on which sites new virtual machines are booted. The Requirement Adapters report about the utilization and resource requirements of the attached batch systems. The Site Adapter is responsible to manage the lifetime of virtual machines on a cloud site and the Integration Adapters ensure that newly booted machines are integrated into the batch system.}
  \label{fig-roced}
\end{center}
\end{figure*}

%$\to$ CMS Karlsruhe
Many capable batch systems exist today and they can be interfaced to virtualization providers using the cloud meta-scheduler \Roced (Responsive On-demand Cloud Enabled Deployment) which has been developed at the KIT since 2010~\cite{ROCED}. \Roced is written in a modular
fashion in python and the interfaces to batch systems and cloud sites
are implemented as so-called \textit{Adapters}. This makes \Roced
independent of specific user groups or workflows. It provides a
scheduling core which collects the current requirement of computing
resources and decides if virtual machines need to be started or can be
stopped. One or more Requirement Adapters report the current queue
status of batch systems to the central scheduling core. Currently,
Requirement Adapters are implemented for the Slurm, Torque/Moab, HTCondor
and GridEngine batch systems. The Site Adapters allow \Roced to start,
stop, and monitor virtual machines on multiple cloud
sites. Implementations exist for Amazon EC2, OpenStack, OpenNebula and
Moab-based virtualization at HPC centers. Special care has been put
into the resilience of \Roced: it can automatically terminate
non-responsive machines and restart virtual machines in case some
machines have dropped out. This allows VM setups orchestrated by \Roced with thousands of virtual machines and many tens of thousands of jobs to run in production environments.
The modular design of \Roced is shown in Fig.~\ref{fig-roced}.

\subsection{Using \HTCondor as front-end scheduler}\label{sec:ROCED:HTCondor}
%$\to$ CMS Karlsruhe
\input{include/HTCondor.tex}

\subsection{Using \Slurm as front-end scheduler}
%$\to$ ATLAS Freiburg\\
\input{include/Slurm.tex}


\section{Analysis of performance and usage}

The ROCED-based solution described above has been implemented and put into production by the research groups at the University of Freiburg (Institute of Physics) and the KIT (Institute of Experimental
Particle Physics). To prove the usefulness of this approach
statistical analyses of the performance of the virtualized setup both
in terms of CPU benchmarks and usage statistics have been conducted.

\subsection{Benchmarks}
%$\to$ ATLAS Freiburg\\
\input{include/Benchmarks.tex}



%\subsection{Production of simulation data}
%%$\to$ATLAS Freiburg
%\input{include/Simulationtests.tex}
%
%\subsection{Data analysis}
%$\to$ATLAS Freiburg 
%
\subsection{Usage statistics}
Fig. \ref{fig-frplots} shows the utilization of virtual machines which were orchestrated by \Roced depending on the resource demands of the users of the KIT group.
At peak times, up to 9000 virtual cores were filled with user jobs, consuming more than a half of the initial 16000 \NEMO cores.

\begin{figure}
\begin{center}
  \includegraphics[width=1.0\linewidth]{figures/NEMO_KIT_utiliztion.pdf}
  \caption{Utilization of the shared HPC system by booted virtual machines. Up to 9000 virtual cores were in use at peak times. The fluctuations in the utilization reflects the patterns of the submission of jobs by the CMS users at the physics institute in Karlsruhe. The number of draining slots displays the amount of job slots still processing jobs while the rest of the node's slot are already empty.}
  \label{fig-frplots}
\end{center}
\end{figure}

The usage of the hybrid cluster model is presented in Fig.~\ref{fig-nodeusage}.
The diagram shows the shared usage of \NEMO's cluster nodes running either
bare-metal or virtualized jobs. The part of the cluster which runs virtualized
jobs changes dynamically from job to job, since the VREs are started by
a standard bare-metal job.

\begin{figure}[ht]
\centering
  \includegraphics[width=\linewidth]{figures/NodeUsage_2016-09_2018-09.pdf}
  \caption{Estimated usage of the \NEMO cluster in the time from September 2016
    to September 2018. The orange bars indicate the usage by jobs
    running directly in the hosts' operating system, while the blue bars are jobs
    running in virtual machines. The decrease of VRE jobs is partially explained
    by an increasing number of bare-metal jobs submitted.}
  \label{fig-nodeusage}
\end{figure}

At the beginning the cluster was only containing the operating system and some
basic development tools. Scientific software was added after the cluster was
already in production mode. Since the VRE for the CMS collaboration was already
available when the \NEMO cluster started, it could already use the whole cluster
while other groups still had to migrate from other ressources. This explains the
high usage by VREs in the first months of
operation. With more and more software being available for bare-metal usage the
type of job stared to increase in numbers and the fraction of VRE
jobs decreased. A longer job queue and other remote cluster ressources are
another explanation, since ROCED dynamically queues VRE jobs depending on the
queue length on the remote sites.

% This figure is only an estimate because VRE
% projects are not forced to use VREs and therefore could run bare-metal jobs as
% well.

% The shared usage of the \NEMO cluster between jobs running in virtual
% machines and within the cluster operating system is illustrated in
% Figure~\ref{fig-nodeusage}. It shows the relative contributions of CPU hours used by jobs running
% direcly in the hosts' operating system or in virtual machines per month.
% (Ist das wirklich so angegeben?)
% The figure shows that the shares of bare-metal and virtualized jobs is
% completely flexible and can be adapted to the current needs and
% priorities of the user community.


\section{Conclusions and Outlook}
\input{include/Conclusions}

%%% REDUNDANT
% A system for the dynamic, on-demand provisioning of virtual machines
% to run jobs in a high energy physics context on an external, not
% dedicated resource as realized at the HPC
% cluster \NEMO at University of Freiburg has been described.
% Reasons for the need for an interface between the schedulers of the host system
% and the external system from which requests are sent have been
% explained.
% The performance and usage have been analyzed for selected cases.
%
% This approach can be generalized to other platforms and possibly also
% other forms of virtualized environments (containers).
















%\subsection{Subsection title}
%\label{sec:2}
%as required. Don't forget to give each section
%and subsection a unique label (see Sect.~\ref{sec:1}).

%\paragraph{Paragraph headings} Use paragraph headings as needed.
%\begin{equation}
%a^2+b^2=c^2
%\end{equation}
%
%% For one-column wide figures use
%\begin{figure}
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
%  \includegraphics{example.eps}
%% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:1}       % Give a unique label
%\end{figure}
%%
%% For two-column wide figures use
%\begin{figure*}
%% Use the relevant command to insert your figure file.
%% For example, with the graphicx package use
%  \includegraphics[width=0.75\textwidth]{example.eps}
%% figure caption is below the figure
%\caption{Please write your figure caption here}
%\label{fig:2}       % Give a unique label
%\end{figure*}
%
%% For tables use
%\begin{table}
%% table caption is above the table
%\caption{Please write your table caption here}
%\label{tab:1}       % Give a unique label
%% For LaTeX tables use
%\begin{tabular}{lll}
%\hline\noalign{\smallskip}
%first & second & third  \\
%\noalign{\smallskip}\hline\noalign{\smallskip}
%number & number & number \\
%number & number & number \\
%\noalign{\smallskip}\hline
%\end{tabular}
%\end{table}


\begin{acknowledgements}
This research is supported by the Ministry of Science, Research and the Arts Baden-W\"urttemberg through the bwHPC grant
and by the German Research Foundation (DFG) through grant no INST
39/963-1 FUGG for the bwForCluster NEMO.
The work of F.B. and T.H. was supported by the Virtual Open Science
Collaboration Environment (ViCE) project MWK 34-7547.221 funded by the
Ministry of Science, Research and the Arts Baden-W\"urttemberg.
The work of U.S. was supported by  the Federal Ministry of Education
and Research within the project 05H15VFCA1
``Higgs-Physik mit dem und Grid-Computing f\"ür das ATLAS-Experiment
am LHC''.
The work of T.H. was supported by the Federal Ministry of Education
and Research within the project 05H15VKCCA ``Physik bei h\"ochsten Energien mit dem CMS-Experiment
    am LHC''.
\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
%\bibliography{}   % name your BibTeX data base

% Non-BibTeX users please use
\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
\bibitem{HLLHCcompneeds}
  ATLAS Collaboration (2018) Estimation of ATLAS computing needs vs
  predictable increase on a flat budget. ATLAS Public results
  \url{https://twiki.cern.ch/twiki/pub/AtlasPublic/ComputingandSoftwarePublicResults/cpuHLLHC.pdf},
  accessed 2018-09-19

\bibitem{wlcg}	Eck C, Knobloch J, Robertson L, Bird I, Bos K, Brook
  N, Düllmann D, Fisk I, Foster D, Gibbard B, Grandi C, Grey F, Harvey
  J, Heiss A, Hemmer F, Jarp S, Jones R, Kelsey D, Lamanna M, Marten
  H, Mato-Vila P, Ould-Saada F, Panzer-Steindel B, Perini L, Schutz Y,
  Schwickerath U, Shiers J, Wenaus T (2005) LHC Computing Grid: Technical Design Report, 
  CERN-LHCC-2005-024


\bibitem{nemo} bwForCluster \NEMO \url{https://www.hpc.uni-freiburg.de/nemo}, accessed 2018-10-21
  
\bibitem{Openstack}
OpenStack: Open Source Cloud Computing Software
\url{https://www.openstack.org/}, accessed 2018-07-03

\bibitem{ROCED}
\Roced Cloud Meta-Scheduler project website
\url{https://github.com/roced-scheduler/ROCED}, accessed 2018-07-03

\bibitem{Omnipath}
 Intel (2015) Omni-Path:
Intel Architects High Performance Computing System Designs to Bring
Power of Supercomputing Mainstream,
\texttt{\href{https://newsroom.intel.com/news-releases/intel-architects-high-performance-computing-system-designs-to-bring-power-of-supercomputing-mainstream/}{https://newsroom.intel.com/news-releases/intel-\\architects-high-performance-computing-system-\\designs-to-bring-power-of-supercomputing-mainstream}}, accessed 2018-09-20
%https://newsroom.intel.com/news-releases/intel-architects-high-performance-computing-system-designs-to-bring-power-of-supercomputing-mainstream
\bibitem{BeeGFS}
BeeGFS Parallel Cluster File system:
\url{https://www.beegfs.io/content/}, accessed 2018-09-20

\bibitem{hpc-symp:2016}
 von Suchodoletz D,  Wiebelt B,  Meier K,  Janczyk M (2016)
  Flexible HPC: bwForCluster \NEMO,
  Proceedings of the 3rd bwHPC-Symposium % in Heidelberg

\bibitem{VRE2017}
 Janczyk M,  Wiebelt B, von Suchodoletz D (2017)
  Virtualized Research Environments on the bwForCluster \NEMO,
  Proceedings of the 4th bwHPC Symposium 


\bibitem{Moab}
Adaptive Computing Moab
\url{http://www.adaptivecomputing.com/moab-hpc-basic-edition/}, accessed 2018-07-03

\bibitem{packer}
Packer: tool for creating machine and container images for multiple platforms from a single source configuration. 
\url{https://www.packer.io/}, accessed 2018-07-03

\bibitem{kickstart}
 Redhat: Kickstart
\url{https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/installation_guide/ch-kickstart2}, accessed 2018-07-03

\bibitem{puppet}
Puppet Enterprise. IT automation for cloud, security, and DevOps.
\url{https://puppet.com/}, accessed 2018-07-03


\bibitem{OZ}
Oz image generation toolkit
\url{https://github.com/clalancette/oz}, accessed 2018-07-03

\bibitem{awsbatch}
Amazon AWS Batch
\url{https://aws.amazon.com/batch/}, accessed 2018-07-03

\bibitem{HTCondor}
HTCondor workload manager
\url{https://research.cs.wisc.edu/htcondor/}, accessed 2018-07-03

\bibitem{HTCondorCCB}
HTCondor Connection Brokering
\url{http://research.cs.wisc.edu/htcondor/manual/v8.6/3_9Networking_includes.html}, accessed 2018-07-03

\bibitem{Slurm}
  Slurm workload manager
  \url{https://slurm.schedmd.com}, accessed 2018-07-03
    
\bibitem{SlurmElastic}
Slurm Elastic Computing
\url{https://slurm.schedmd.com/elastic_computing.html}, accessed 2018-07-03


\bibitem{Hepspec} HEPiX Benchmarking Working Group:
\url{https://twiki.cern.ch/twiki/bin/view/FIOgroup/TsiBenchHEPSPEC}, accessed 2018-01-29


\bibitem{Alef:2017jyx}
Alef M  et al. (2017) Benchmarking cloud resources for HEP,
J.\ Phys.\ Conf.\ Ser.\  {\bf 898} (2017) no.9,  092056.
doi:10.1088/1742-6596/898/9/092056
%%CITATION = doi:10.1088/1742-6596/898/9/092056;%%

\bibitem{DeSalvo:2010zza}
De Salvo A and Brasolin F (2010)
Benchmarking the ATLAS software through the kit validation engine,
J.\ Phys.\ Conf.\ Ser.\  {\bf 219} (2010) 042037.
doi:10.1088/1742-6596/219/4/042037
%%CITATION = doi:10.1088/1742-6596/219/4/042037;%%
%4 citations counted in INSPIRE as of 11 Sep 2018

\bibitem{Agostinelli:2002hh}                               
Agostinelli S  et al. [GEANT4 Collaboration] (2003),        
GEANT4: A Simulation toolkit,                          
Nucl.\ Instrum.\ Meth.\ A {\bf 506} (2003) 250.            
doi:10.1016/S0168-9002(03)01368-8                          
%%CITATION = doi:10.1016/S0168-9002(03)01368-8;%%          
%9602 citations counted in INSPIRE as of 11 Sep 2018       

\bibitem{SL6}
Fermilab and CERN,
Scientific Linux 6,
\url{http://www.scientificlinux.org/}, accessed 2018-11-03

\bibitem{CentOS7}
The CentOS Project,
CentOS Linux 7,
\url{https://www.centos.org/}, accessed 2018-11-03

\bibitem{online:modules}
Environment Modules open source project,
Environment Modules,
\url{http://modules.sourceforge.net}, accessed 2019-02-07

%title={Flexible HPC: bwForCluster NEMO},
%authors={Dirk von Suchodoletz and Bernd Wiebelt and Konrad Meier and
%Michael Janczyk},
%editors={Richling, Sabine and Baumann, Martin and Heuveline, Vincent},
%booktitle={Proceedings of the 3rd bwHPC-Symposium: Heidelberg 2016},
%publisher={heiBOOKS},
%year={2017},
%DOI={10.11588/heibooks.308.418},
%url={http://books.ub.uni
%heidelberg.de/heibooks/reader/download/308/308-4-79237-1-10-20171002.pdf}
%}

% {VRE2017,
%   title={Virtualized Research Environments on the bwForCluster NEMO},
%   booktitle={Proceedings of the 4th bwHPC Symposium October 4th, 2017,
% Alte Aula Eberhard Karls Universit{\"a}t T{\"u}bingen},
%   editors={Jens Kr{\"uger and Thomas Walter},
%   author={Janczyk, Michael and Wiebelt, Bernd and von Suchodoletz, Dirk},
%   pages={37--40},
%   year={2017},
%   url={http://hdl.handle.net/10900/83815},
%   doi={http://dx.doi.org/10.15496/publikation-25205}
% }

%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
% etc
\end{thebibliography}

\end{document}
% end of file template.tex

